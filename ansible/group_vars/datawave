# Things you'll likely consider overriding at some point...

#dw_checkout_version: release/version2.6
#dw_project_version: 2.6.25-SNAPSHOT
#dw_repo: "https://github.com/me/datawave"
#dw_build_profile: testenv

#dw_wildfly_args: "--debug"

#dw_all_ingest_data_types: [datatype1,datatype2]
#dw_accumulo_root_auths: [A,B,C,D,E,F,G]

# By default, dw_num_shards will be set to the number of available
# worker nodes. This may result in an insufficient number of intra-day
# shard partitions for large datasets in which all documents in the 
# corpus are ingested into the same date partition (YYYYMMDD_*)
# E.g., Wikipedia

#dw_num_shards: 300

# A few directories that you should ensure have adequate free space...

# ~3GB on the proxy host to build and assemble all DW artifacts...
#dw_clone_dir: "/media/ephemeral0/datawave-source"

# ~3GB on the proxy host for the local m2 repo...
#dw_m2_repo_dir: "/media/ephemeral0/.m2/repository"

# ~20GB on the proxy host for the compressed Wikipedia dump and index files...
#wiki_dump_local_dir: /media/ephemeral0/{{ wiki_dump_lang }}wiki-{{ wiki_dump_date }}

# Ingest tuning (to control the number of mappers and concurrent jobs)

dw_live_max_files_per_job: 10
dw_live_max_ingest_jobs: 5


